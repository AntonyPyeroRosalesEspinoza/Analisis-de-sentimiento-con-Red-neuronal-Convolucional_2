{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonyPyeroRosalesEspinoza/Analisis-de-sentimiento-con-Red-neuronal-Convolucional_2/blob/main/Analisis_de_sentimiento_con_Red_neuronal_Convolucional_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_j3fr9SNih"
      },
      "source": [
        "Permisos de google para Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1nnREGwSDOK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pItNN6ioja3v",
        "outputId": "3696d6f8-e168-4bd7-ac4b-a9eb1cebf10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojG5xA1xSDKz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import multiprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Amf3WVIkDAp"
      },
      "source": [
        "**LEER ARCHIVO EXCEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dYfdL6rakSlb",
        "outputId": "ef89d16d-1a06-448b-9e62-b3785896780d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Sentimiento                                              Tweet\n",
              "0              0  there is no language for such abysms of shriek...\n",
              "1              1  I see no difference between the two movements,...\n",
              "2              1  I've been making these Balderich comic pages h...\n",
              "3              0                               My Artwork vs AI art\n",
              "4              0  Due to copyright concerns, stock image service...\n",
              "..           ...                                                ...\n",
              "756            1  You've been so polite, composed, and informati...\n",
              "757            0  If someone codes an AI to play a competitive g...\n",
              "758            0  A restaurant customer also curates the end res...\n",
              "759            0  I did, you just won't accept how it actually i...\n",
              "760            0  You mean the ones made by Ben Dunn? AI no useful.\n",
              "\n",
              "[761 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a8f4648-2db6-464b-84ed-393d8cfbdf04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentimiento</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>there is no language for such abysms of shriek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I see no difference between the two movements,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I've been making these Balderich comic pages h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>My Artwork vs AI art</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Due to copyright concerns, stock image service...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>1</td>\n",
              "      <td>You've been so polite, composed, and informati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>0</td>\n",
              "      <td>If someone codes an AI to play a competitive g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>0</td>\n",
              "      <td>A restaurant customer also curates the end res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>0</td>\n",
              "      <td>I did, you just won't accept how it actually i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>0</td>\n",
              "      <td>You mean the ones made by Ben Dunn? AI no useful.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>761 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a8f4648-2db6-464b-84ed-393d8cfbdf04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9a8f4648-2db6-464b-84ed-393d8cfbdf04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9a8f4648-2db6-464b-84ed-393d8cfbdf04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b393ed8-74b7-4122-8ca4-0c6e422bb73c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b393ed8-74b7-4122-8ca4-0c6e422bb73c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b393ed8-74b7-4122-8ca4-0c6e422bb73c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "data=pd.read_excel('/content/drive/MyDrive/Informacion1.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNjbTSUUTCbl"
      },
      "outputs": [],
      "source": [
        "#Crear Nueva variable para visualizar el excel\n",
        "my_df=data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3LlezU3DTPJB",
        "outputId": "9e417f59-4564-4c34-d0f0-ac85f87df244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sentimiento                                              Tweet\n",
              "0            0  there is no language for such abysms of shriek...\n",
              "1            1  I see no difference between the two movements,...\n",
              "2            1  I've been making these Balderich comic pages h...\n",
              "3            0                               My Artwork vs AI art\n",
              "4            0  Due to copyright concerns, stock image service..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-284791f1-62f9-4a04-b23f-17f81c93ad07\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentimiento</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>there is no language for such abysms of shriek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I see no difference between the two movements,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I've been making these Balderich comic pages h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>My Artwork vs AI art</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Due to copyright concerns, stock image service...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-284791f1-62f9-4a04-b23f-17f81c93ad07')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-284791f1-62f9-4a04-b23f-17f81c93ad07 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-284791f1-62f9-4a04-b23f-17f81c93ad07');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8da136a7-6613-42e7-aa0c-d9494664aeed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8da136a7-6613-42e7-aa0c-d9494664aeed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8da136a7-6613-42e7-aa0c-d9494664aeed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "#Visualizar 5 primeras líneas del archivo excel\n",
        "my_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl8ygSkHTn1Z",
        "outputId": "499b48dc-eab5-4924-f2da-0ee6e5d2eeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 761 entries, 0 to 760\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Sentimiento  761 non-null    int64 \n",
            " 1   Tweet        761 non-null    object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 12.0+ KB\n"
          ]
        }
      ],
      "source": [
        "#Elimina valores nulos en las filas\n",
        "my_df.dropna(inplace=True)\n",
        "my_df.reset_index(drop=True,inplace=True)\n",
        "my_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoadD2NCUAa0"
      },
      "outputs": [],
      "source": [
        "#Separacion de datos en variables segun la celda Tweet y Sentimiento\n",
        "x = my_df.Tweet\n",
        "y = my_df.Sentimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4olrIFaUoZl"
      },
      "source": [
        "**Dividir los datos en entrenamiento y\n",
        "prueba**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPDbgGHpVts9"
      },
      "outputs": [],
      "source": [
        "#Dividiendo los datos para el entrenamiento\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "g3M-r1vbWUCS",
        "outputId": "3eb399da-b239-459c-fe1f-e6cd1d424378"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Los datos tiene un total de 608 entradas con 55.59% negativos, 44.41% positivos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "#Datos para entrenamiento / train\n",
        "\"Los datos tiene un total de {0} entradas con {1:.2f}% negativos, {2:.2f}% positivos\".format(len(x_train),\n",
        "                                                                                      (len(x_train[y_train == 0]) / (len(x_train)*1.0))*100,\n",
        "                                                                                      (len(x_train[y_train == 1]) / (len(x_train)*1.0))*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "V4IxH6VLaZf5",
        "outputId": "dab67628-f06f-472a-989c-47241b7e3351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Los datos tiene un total de 153 entradas con 60.13% negativos, 39.87% positivos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "#Datos prueba / test\n",
        "\"Los datos tiene un total de {0} entradas con {1:.2f}% negativos, {2:.2f}% positivos\".format(len(x_test),\n",
        "                                                                                      (len(x_test[y_test == 0]) / (len(x_test)*1.0))*100,\n",
        "                                                                                      (len(x_test[y_test == 1]) / (len(x_test)*1.0))*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2yXzNnJmzCi"
      },
      "source": [
        "**Preparacion de la capa de incrustación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GShNIizRm4Qv"
      },
      "outputs": [],
      "source": [
        "#etiquetar los textos para convertirlo en formato para Word2Vec\n",
        "def labelize_tweets_ug(tweets,label):\n",
        "    result = []\n",
        "    prefix = label\n",
        "    for i, t in zip(tweets.index, tweets):\n",
        "      result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG-qQH_jn6Yi"
      },
      "outputs": [],
      "source": [
        "#Combina los datos en un solo conjunto 'all'\n",
        "all_x = pd.concat([x_train,x_test])\n",
        "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoFwvcqkoHzS",
        "outputId": "60deea84-0c9c-4bdf-fec4-bc590a3255ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['You', 'can', 'do', 'what', 'many', 'of', 'them', 'cannot.', 'Create', 'entire', 'worlds,', 'Universes', 'using', 'only', 'words.', 'I', 'haven’t', 'finished', 'the', 'Audible', 'of', 'Shadow', 'of', 'the', 'Conqueror', '(yet).', 'What', 'I’ve', 'heard', 'so', 'far', 'I', 'feel', 'like', 'I’m', 'seeing', 'those', 'characters', 'and', 'the', 'entire', 'world', 'you', 'created', 'in', 'my', 'mind.', 'Whether', '“they”', 'like', 'it', 'or', 'not', 'you', 'are', 'an', 'artist', 'my', 'friend.'], tags=['all_214'])]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "#Muestra las etiquetas de los 5 elementos\n",
        "all_x_w2v[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18gQIaDCoXkA",
        "outputId": "d7d0f0fb-4cf3-4372-e3b1-244f6555ff68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1431.99it/s]\n"
          ]
        }
      ],
      "source": [
        "#Configura y entrena el modelo Word2Vec para el procesamiento de texto (Se obtiene vocabulario con datos etiquetados)\n",
        "cores = multiprocessing.cpu_count()\n",
        "model_ug_sg = Word2Vec(sg=1, vector_size=200, negative=5, window=2,min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrHW2p8cpw4q",
        "outputId": "14fe928d-45b3-4c19-ed87-c0ae5520a15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8793.09it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8525.01it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 7825.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8289.14it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9664.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 854.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8867.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 7108.99it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9320.68it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9039.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 7898.88it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 9218.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 269 ms, sys: 23.9 ms, total: 293 ms\n",
            "Wall time: 395 ms\n"
          ]
        }
      ],
      "source": [
        "#Entrenamiento del modelo word2vec\n",
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "    model_ug_sg.alpha -= 0.002\n",
        "    model_ug_sg.min_alpha = model_ug_sg.alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5jKO7FJqRLB"
      },
      "outputs": [],
      "source": [
        "#Guardar modelo Word2Vec\n",
        "model_ug_sg.save('w2v_model_ug_sg.word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJDc434Vq9O8"
      },
      "outputs": [],
      "source": [
        "#Cargar modelo Word2Vec\n",
        "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xug_VswgVxif",
        "outputId": "12bafdfb-469a-4561-d3b8-635e855549af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontro 7 vectores de palabras.\n"
          ]
        }
      ],
      "source": [
        "#Muestra cantidad de vectores del Word2Vec\n",
        "embedding_index = {}\n",
        "for w in model_ug_sg.wv.index_to_key:\n",
        "  embedding_index[w] = model_ug_sg.wv[w]\n",
        "print('Se encontro %s vectores de palabras.' % len(embedding_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wppMLbLBXTXX"
      },
      "outputs": [],
      "source": [
        "#embedding_index['are']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHORXKYYO1Y"
      },
      "outputs": [],
      "source": [
        "#Tokenizar para pasar de letras a secuencia de numeros (numero maximo 10000 palabras)\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "sequences = tokenizer.texts_to_sequences(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmfoECm_Y9fx",
        "outputId": "de93b352-3b03-4627-f216-ab0cc4c2c819"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2046"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "#Total de palabras unicas en el diccionario\n",
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lokkUjOZBuP"
      },
      "source": [
        "**Mostrar 5 tweets / vectores de palabras de los datos de entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EKulGZncIZt",
        "outputId": "265f6927-606e-41f5-a5d1-6ca0ca2d3988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can do what many of them cannot. Create entire worlds, Universes using only words. I haven’t finished the Audible of Shadow of the Conqueror (yet). What I’ve heard so far I feel like I’m seeing those characters and the entire world you created in my mind. Whether “they” like it or not you are an artist my friend.\n",
            "Dawg it's not a tool, it's literally a crutch for you to paint your turds gold and remain delusional just get better at drawing, homie\n",
            "indeed but also I feel like this was like the original intention, and then capitalism got greedy, as it is wont. It's so dumb that they way you're using it here, which is the RIGHT ETHICAL way, is now basically rebellion\n",
            "not legally, but actual artists understand that biting a style is a massive no-no in the artistic world. This is not to get confused with taking inspiration from another's style; however, straight up making your work look like someone else's is not something you do in art ia.\n",
            "The studio's A.I. proposal to SAG-AFTRA included scanning a background actor's likeness for one day's worth of pay and using their likeness forever in any form without any pay or consent.\n"
          ]
        }
      ],
      "source": [
        "for x in x_train[:5]:\n",
        "  print (x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH4kQirXcXUl"
      },
      "source": [
        "**Mostrar 5 tweets secuenciales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAgEsYgtdds8",
        "outputId": "f6950802-2525-46a3-b4a3-cf0b79a6cc7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[9,\n",
              "  36,\n",
              "  40,\n",
              "  33,\n",
              "  104,\n",
              "  10,\n",
              "  88,\n",
              "  361,\n",
              "  189,\n",
              "  416,\n",
              "  736,\n",
              "  737,\n",
              "  45,\n",
              "  73,\n",
              "  574,\n",
              "  7,\n",
              "  738,\n",
              "  417,\n",
              "  1,\n",
              "  739,\n",
              "  10,\n",
              "  740,\n",
              "  10,\n",
              "  1,\n",
              "  741,\n",
              "  165,\n",
              "  33,\n",
              "  312,\n",
              "  742,\n",
              "  42,\n",
              "  275,\n",
              "  7,\n",
              "  119,\n",
              "  23,\n",
              "  144,\n",
              "  276,\n",
              "  153,\n",
              "  313,\n",
              "  4,\n",
              "  1,\n",
              "  416,\n",
              "  174,\n",
              "  9,\n",
              "  154,\n",
              "  15,\n",
              "  31,\n",
              "  743,\n",
              "  744,\n",
              "  745,\n",
              "  23,\n",
              "  11,\n",
              "  51,\n",
              "  18,\n",
              "  9,\n",
              "  17,\n",
              "  21,\n",
              "  44,\n",
              "  31,\n",
              "  418],\n",
              " [1450,\n",
              "  35,\n",
              "  18,\n",
              "  5,\n",
              "  99,\n",
              "  35,\n",
              "  314,\n",
              "  5,\n",
              "  1451,\n",
              "  13,\n",
              "  9,\n",
              "  3,\n",
              "  315,\n",
              "  26,\n",
              "  1452,\n",
              "  1453,\n",
              "  4,\n",
              "  1454,\n",
              "  1455,\n",
              "  27,\n",
              "  48,\n",
              "  113,\n",
              "  49,\n",
              "  128,\n",
              "  1456],\n",
              " [244,\n",
              "  20,\n",
              "  129,\n",
              "  7,\n",
              "  119,\n",
              "  23,\n",
              "  16,\n",
              "  55,\n",
              "  23,\n",
              "  1,\n",
              "  222,\n",
              "  746,\n",
              "  4,\n",
              "  100,\n",
              "  747,\n",
              "  316,\n",
              "  748,\n",
              "  28,\n",
              "  11,\n",
              "  8,\n",
              "  749,\n",
              "  35,\n",
              "  42,\n",
              "  750,\n",
              "  14,\n",
              "  24,\n",
              "  70,\n",
              "  130,\n",
              "  45,\n",
              "  11,\n",
              "  145,\n",
              "  89,\n",
              "  8,\n",
              "  1,\n",
              "  206,\n",
              "  175,\n",
              "  70,\n",
              "  8,\n",
              "  109,\n",
              "  575,\n",
              "  751],\n",
              " [18,\n",
              "  752,\n",
              "  20,\n",
              "  362,\n",
              "  29,\n",
              "  166,\n",
              "  14,\n",
              "  753,\n",
              "  5,\n",
              "  131,\n",
              "  8,\n",
              "  5,\n",
              "  754,\n",
              "  39,\n",
              "  39,\n",
              "  15,\n",
              "  1,\n",
              "  84,\n",
              "  174,\n",
              "  16,\n",
              "  8,\n",
              "  18,\n",
              "  3,\n",
              "  48,\n",
              "  755,\n",
              "  12,\n",
              "  223,\n",
              "  576,\n",
              "  41,\n",
              "  756,\n",
              "  131,\n",
              "  419,\n",
              "  245,\n",
              "  114,\n",
              "  101,\n",
              "  26,\n",
              "  46,\n",
              "  155,\n",
              "  23,\n",
              "  156,\n",
              "  757,\n",
              "  8,\n",
              "  18,\n",
              "  102,\n",
              "  9,\n",
              "  40,\n",
              "  15,\n",
              "  6,\n",
              "  22],\n",
              " [1,\n",
              "  758,\n",
              "  5,\n",
              "  7,\n",
              "  759,\n",
              "  3,\n",
              "  760,\n",
              "  761,\n",
              "  762,\n",
              "  763,\n",
              "  5,\n",
              "  246,\n",
              "  764,\n",
              "  363,\n",
              "  13,\n",
              "  62,\n",
              "  765,\n",
              "  766,\n",
              "  10,\n",
              "  277,\n",
              "  4,\n",
              "  45,\n",
              "  53,\n",
              "  363,\n",
              "  420,\n",
              "  15,\n",
              "  115,\n",
              "  207,\n",
              "  176,\n",
              "  115,\n",
              "  277,\n",
              "  51,\n",
              "  278]]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "#Muestra los tweets en secuencia / numeral\n",
        "sequences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNc2NR9PeVNR"
      },
      "outputs": [],
      "source": [
        "#Length para contener la longitud de cada texto\n",
        "length = []\n",
        "for x in x_train:\n",
        "  length.append(len(x.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP-3K8cGib7a"
      },
      "source": [
        "**Midiendo la longitud maxima (ejemplo: si es 51 entonces ligeramente sea superior como 55)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2lwDML0ioTf",
        "outputId": "a0808276-d701-40fe-885c-ef6edeff4f29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "#Longitud maxima\n",
        "max(length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7wTfif5itzA",
        "outputId": "7c12ee88-0481-44b1-d383-a01c3d343bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de datos del Tensor: (608, 100)\n"
          ]
        }
      ],
      "source": [
        "#Agrega padding a la sequencia de datos entrenamiento (ejemplo: padding 3 para solo uno: 0,0,1 rellena con 0 los vacios)\n",
        "#maximocam para cambiar el tamaño maximo de la longitud\n",
        "maximocam= 100\n",
        "x_train_seq = pad_sequences(sequences, maxlen = maximocam)\n",
        "print('Tamaño de datos del Tensor:', x_train_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHRlXwfdkde0",
        "outputId": "6a770c2c-0e4c-41ae-cf38-2e50f40061d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    9,   36,   40,\n",
              "          33,  104,   10,   88,  361,  189,  416,  736,  737,   45,   73,\n",
              "         574,    7,  738,  417,    1,  739,   10,  740,   10,    1,  741,\n",
              "         165,   33,  312,  742,   42,  275,    7,  119,   23,  144,  276,\n",
              "         153,  313,    4,    1,  416,  174,    9,  154,   15,   31,  743,\n",
              "         744,  745,   23,   11,   51,   18,    9,   17,   21,   44,   31,\n",
              "         418],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0, 1450,   35,\n",
              "          18,    5,   99,   35,  314,    5, 1451,   13,    9,    3,  315,\n",
              "          26, 1452, 1453,    4, 1454, 1455,   27,   48,  113,   49,  128,\n",
              "        1456],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,  244,   20,  129,    7,  119,   23,   16,\n",
              "          55,   23,    1,  222,  746,    4,  100,  747,  316,  748,   28,\n",
              "          11,    8,  749,   35,   42,  750,   14,   24,   70,  130,   45,\n",
              "          11,  145,   89,    8,    1,  206,  175,   70,    8,  109,  575,\n",
              "         751],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,   18,  752,   20,  362,\n",
              "          29,  166,   14,  753,    5,  131,    8,    5,  754,   39,   39,\n",
              "          15,    1,   84,  174,   16,    8,   18,    3,   48,  755,   12,\n",
              "         223,  576,   41,  756,  131,  419,  245,  114,  101,   26,   46,\n",
              "         155,   23,  156,  757,    8,   18,  102,    9,   40,   15,    6,\n",
              "          22],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    1,  758,    5,    7,  759,    3,  760,  761,  762,  763,\n",
              "           5,  246,  764,  363,   13,   62,  765,  766,   10,  277,    4,\n",
              "          45,   53,  363,  420,   15,  115,  207,  176,  115,  277,   51,\n",
              "         278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "#Muestra la secuencia con padding\n",
        "x_train_seq[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQhMHvUallzn"
      },
      "outputs": [],
      "source": [
        "#Agrega padding a la sequencia de datos prueba (ejemplo: padding 3 para solo uno: 0,0,1 rellena con 0 los vacios)\n",
        "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_test_seq = pad_sequences(sequences_test,maxlen = maximocam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXoDmX5fmHdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8c7334-cbd9-4f5f-b298-825c5c653ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "you\n",
            "of\n",
            "like\n",
            "my\n",
            "entire\n"
          ]
        }
      ],
      "source": [
        "#Matriz de pesos inicial para la capa de embedding\n",
        "import numpy as np\n",
        "\n",
        "num_words = 10000\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i >= num_words:\n",
        "    continue\n",
        "  embedding_vector = embedding_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    print(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFiYGXzYbEf_",
        "outputId": "d7b71e2b-fd8d-47f4-a664-4467e9e45bb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "#Comprobacion de la primera palabra de la matriz con la palabra ('the')\n",
        "np.array_equal(embedding_matrix[1], embedding_index.get('of'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igg0Vuf6bVwv"
      },
      "outputs": [],
      "source": [
        "seed = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvT8rKMVcBqt"
      },
      "outputs": [],
      "source": [
        "#Concatena la secuencia de numeros / validacion cruzada KFold para dividir en 5 para diferentes iteraciones\n",
        "x_new = np.concatenate((x_train_seq, x_test_seq), axis=0)\n",
        "y_new = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=100, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR7KRI_IccmO",
        "outputId": "f48c93c8-9722-4787-cbed-0e7bf7a9278b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "19/19 - 4s - loss: 0.7233 - accuracy: 0.5822 - val_loss: 0.5984 - val_accuracy: 0.8562 - 4s/epoch - 205ms/step\n",
            "Epoch 2/5\n",
            "19/19 - 3s - loss: 0.4481 - accuracy: 0.9046 - val_loss: 0.3893 - val_accuracy: 0.9150 - 3s/epoch - 137ms/step\n",
            "Epoch 3/5\n",
            "19/19 - 3s - loss: 0.1717 - accuracy: 0.9967 - val_loss: 0.2083 - val_accuracy: 0.9346 - 3s/epoch - 138ms/step\n",
            "Epoch 4/5\n",
            "19/19 - 2s - loss: 0.0456 - accuracy: 0.9967 - val_loss: 0.1423 - val_accuracy: 0.9477 - 2s/epoch - 97ms/step\n",
            "Epoch 5/5\n",
            "19/19 - 2s - loss: 0.0142 - accuracy: 0.9984 - val_loss: 0.1267 - val_accuracy: 0.9477 - 2s/epoch - 93ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c4725c8af20>"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "#Creacion del modelo con embeddings\n",
        "model_ptw2v = Sequential()\n",
        "e = Embedding(10000, 200, input_length = maximocam)\n",
        "model_ptw2v.add(e)\n",
        "model_ptw2v.add(Flatten())\n",
        "model_ptw2v.add(Dense(256, activation='relu'))\n",
        "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
        "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=5, batch_size=32, verbose= 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilAD2B-0deIX",
        "outputId": "1989724b-a286-4982-a94d-599e1595fe01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "19/19 - 3s - loss: 0.6589 - accuracy: 0.6036 - val_loss: 0.5173 - val_accuracy: 0.8039 - 3s/epoch - 142ms/step\n",
            "Epoch 2/5\n",
            "19/19 - 2s - loss: 0.2207 - accuracy: 0.9786 - val_loss: 0.1831 - val_accuracy: 0.9216 - 2s/epoch - 96ms/step\n",
            "Epoch 3/5\n",
            "19/19 - 2s - loss: 0.0132 - accuracy: 0.9984 - val_loss: 0.1758 - val_accuracy: 0.9346 - 2s/epoch - 89ms/step\n",
            "Epoch 4/5\n",
            "19/19 - 2s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9346 - 2s/epoch - 123ms/step\n",
            "Epoch 5/5\n",
            "19/19 - 2s - loss: 7.4526e-04 - accuracy: 1.0000 - val_loss: 0.1653 - val_accuracy: 0.9477 - 2s/epoch - 130ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c4731e6e980>"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "#Creacion del modelo con embbedings preentrenado\n",
        "model_ptw2v = Sequential()\n",
        "e = Embedding(10000, 200, weights=[embedding_matrix], input_length = maximocam, trainable=True)\n",
        "model_ptw2v.add(e)\n",
        "model_ptw2v.add(Flatten())\n",
        "model_ptw2v.add(Dense(256, activation='relu'))\n",
        "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
        "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=5, batch_size=32, verbose= 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCj13BQDd9dE"
      },
      "source": [
        "**Red Neuronal Convolucional**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIev9cRveFeT",
        "outputId": "7ae5c90c-51c1-4c47-fb86-eeb9c661821e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)    (None, 100, 200)             2000000   ['input_10[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)          (None, 99, 100)              40100     ['embedding_13[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)          (None, 98, 100)              60100     ['embedding_13[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)          (None, 97, 100)              80100     ['embedding_13[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_27 (G  (None, 100)                  0         ['conv1d_27[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_28 (G  (None, 100)                  0         ['conv1d_28[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_29 (G  (None, 100)                  0         ['conv1d_29[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate  (None, 300)                  0         ['global_max_pooling1d_27[0][0\n",
            " )                                                                  ]',                           \n",
            "                                                                     'global_max_pooling1d_28[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_29[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, 256)                  77056     ['concatenate_9[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 256)                  0         ['dense_26[0][0]']            \n",
            "                                                                                                  \n",
            " dense_27 (Dense)            (None, 1)                    257       ['dropout_9[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 1)                    0         ['dense_27[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.src.layers import GlobalMaxPool1D\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, concatenate, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "#lr = 0.0001 y dropout = 0.2 (hiperparametro lr para actualizar los pesos / dropout previene sobreajuste)\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.2)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.0001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S-SI2XE3haD",
        "outputId": "8a2918ca-6df6-4fb1-bbfb-97cb6300d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6784 - accuracy: 0.5729\n",
            "Epoch 1: val_accuracy improved from -inf to 0.68852, saving model to CNN_best_weightds.01-0.6885.hdf1\n",
            "16/16 [==============================] - 5s 207ms/step - loss: 0.6782 - accuracy: 0.5782 - val_loss: 0.6496 - val_accuracy: 0.6885\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5518 - accuracy: 0.8562\n",
            "Epoch 2: val_accuracy improved from 0.68852 to 0.78689, saving model to CNN_best_weightds.02-0.7869.hdf1\n",
            "16/16 [==============================] - 3s 196ms/step - loss: 0.5494 - accuracy: 0.8580 - val_loss: 0.5286 - val_accuracy: 0.7869\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.2904 - accuracy: 0.9521\n",
            "Epoch 3: val_accuracy improved from 0.78689 to 0.90164, saving model to CNN_best_weightds.03-0.9016.hdf1\n",
            "16/16 [==============================] - 3s 192ms/step - loss: 0.2893 - accuracy: 0.9527 - val_loss: 0.3189 - val_accuracy: 0.9016\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy improved from 0.90164 to 0.92623, saving model to CNN_best_weightds.04-0.9262.hdf1\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9262\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.92623 to 0.93443, saving model to CNN_best_weightds.05-0.9344.hdf1\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.1820 - val_accuracy: 0.9344\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0453 - accuracy: 0.9833\n",
            "Epoch 1: val_accuracy improved from 0.93443 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 198ms/step - loss: 0.0448 - accuracy: 0.9835 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.4856e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 8.2763e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 4s 278ms/step - loss: 8.2763e-04 - accuracy: 1.0000 - val_loss: 5.6226e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 6.6529e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 6.6529e-04 - accuracy: 1.0000 - val_loss: 4.9886e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 5.4150e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 5.4150e-04 - accuracy: 1.0000 - val_loss: 4.5100e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.9586e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 5.0293e-04 - accuracy: 1.0000 - val_loss: 4.1224e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.4051e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 4.3801e-04 - accuracy: 1.0000 - val_loss: 3.0802e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.8519e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 3.8534e-04 - accuracy: 1.0000 - val_loss: 2.8388e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.1666e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 3.2186e-04 - accuracy: 1.0000 - val_loss: 2.6360e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.1113e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 3.1077e-04 - accuracy: 1.0000 - val_loss: 2.4724e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7718e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 150ms/step - loss: 2.7718e-04 - accuracy: 1.0000 - val_loss: 2.3138e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5529e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 2.5529e-04 - accuracy: 1.0000 - val_loss: 1.6970e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3293e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 160ms/step - loss: 2.3383e-04 - accuracy: 1.0000 - val_loss: 1.5941e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3636e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 2.3480e-04 - accuracy: 1.0000 - val_loss: 1.5074e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.9560e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 1.9474e-04 - accuracy: 1.0000 - val_loss: 1.4294e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.8742e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 1.8738e-04 - accuracy: 1.0000 - val_loss: 1.3587e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "#Validacion cruzada de Kfold y guarda el peso\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dfZDcxO6D2v",
        "outputId": "7acfd175-94b2-4eb5-8338-b635be24513a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_14 (Embedding)    (None, 100, 200)             2000000   ['input_11[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)          (None, 99, 100)              40100     ['embedding_14[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)          (None, 98, 100)              60100     ['embedding_14[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)          (None, 97, 100)              80100     ['embedding_14[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_30 (G  (None, 100)                  0         ['conv1d_30[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_31 (G  (None, 100)                  0         ['conv1d_31[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_32 (G  (None, 100)                  0         ['conv1d_32[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_30[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_31[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_32[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 256)                  77056     ['concatenate_10[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 256)                  0         ['dense_28[0][0]']            \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 1)                    257       ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 1)                    0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.001 y dropout = 0.2\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.2)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E36mDyo6F_x",
        "outputId": "b2b511ce-f190-416a-a4ab-1835ded893df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.5437\n",
            "Epoch 1: val_accuracy improved from -inf to 0.63934, saving model to CNN_best_weightds.01-0.6393.hdf1\n",
            "16/16 [==============================] - 4s 200ms/step - loss: 0.6869 - accuracy: 0.5453 - val_loss: 0.6562 - val_accuracy: 0.6393\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5858 - accuracy: 0.8875\n",
            "Epoch 2: val_accuracy improved from 0.63934 to 0.73770, saving model to CNN_best_weightds.02-0.7377.hdf1\n",
            "16/16 [==============================] - 3s 192ms/step - loss: 0.5837 - accuracy: 0.8889 - val_loss: 0.5545 - val_accuracy: 0.7377\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.9527\n",
            "Epoch 3: val_accuracy improved from 0.73770 to 0.90984, saving model to CNN_best_weightds.03-0.9098.hdf1\n",
            "16/16 [==============================] - 4s 277ms/step - loss: 0.3445 - accuracy: 0.9527 - val_loss: 0.3384 - val_accuracy: 0.9098\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9938\n",
            "Epoch 4: val_accuracy improved from 0.90984 to 0.92623, saving model to CNN_best_weightds.04-0.9262.hdf1\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.0978 - accuracy: 0.9938 - val_loss: 0.2170 - val_accuracy: 0.9262\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 0.92623\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9180\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0493 - accuracy: 0.9708\n",
            "Epoch 1: val_accuracy improved from 0.92623 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 191ms/step - loss: 0.0489 - accuracy: 0.9712 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 156ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.5239e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 8.7779e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 8.7119e-04 - accuracy: 1.0000 - val_loss: 5.4992e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 6.8673e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 6.8125e-04 - accuracy: 1.0000 - val_loss: 4.7319e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.2392e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 5.2266e-04 - accuracy: 1.0000 - val_loss: 4.1416e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.4130e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 136ms/step - loss: 4.4130e-04 - accuracy: 1.0000 - val_loss: 3.6346e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.1394e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 188ms/step - loss: 4.1394e-04 - accuracy: 1.0000 - val_loss: 2.4818e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.2770e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 3.2711e-04 - accuracy: 1.0000 - val_loss: 2.2080e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.8644e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 2.8584e-04 - accuracy: 1.0000 - val_loss: 1.9880e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4517e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 2.4347e-04 - accuracy: 1.0000 - val_loss: 1.7953e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2022e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 2.2261e-04 - accuracy: 1.0000 - val_loss: 1.6209e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0625e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 2.0587e-04 - accuracy: 1.0000 - val_loss: 1.1880e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9081e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 1.9081e-04 - accuracy: 1.0000 - val_loss: 1.0728e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.6767e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 155ms/step - loss: 1.6767e-04 - accuracy: 1.0000 - val_loss: 9.6921e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.4750e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 1.4750e-04 - accuracy: 1.0000 - val_loss: 8.8269e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.2580e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 1.2580e-04 - accuracy: 1.0000 - val_loss: 8.0503e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ-t26FO6x49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc539f1d-17bc-4eb6-e8cc-6de4ba7b6d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_15 (Embedding)    (None, 100, 200)             2000000   ['input_12[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)          (None, 99, 100)              40100     ['embedding_15[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)          (None, 98, 100)              60100     ['embedding_15[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)          (None, 97, 100)              80100     ['embedding_15[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_33 (G  (None, 100)                  0         ['conv1d_33[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_34 (G  (None, 100)                  0         ['conv1d_34[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_35 (G  (None, 100)                  0         ['conv1d_35[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_33[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_34[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_35[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 256)                  77056     ['concatenate_11[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 256)                  0         ['dense_30[0][0]']            \n",
            "                                                                                                  \n",
            " dense_31 (Dense)            (None, 1)                    257       ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 1)                    0         ['dense_31[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.01 y dropout = 0.2\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.2)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.01)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbAxc1rB61wS",
        "outputId": "36773df2-24d1-451d-a274-6622561806d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.5617\n",
            "Epoch 1: val_accuracy improved from -inf to 0.72951, saving model to CNN_best_weightds.01-0.7295.hdf1\n",
            "16/16 [==============================] - 7s 319ms/step - loss: 0.6870 - accuracy: 0.5617 - val_loss: 0.6724 - val_accuracy: 0.7295\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6196 - accuracy: 0.7521\n",
            "Epoch 2: val_accuracy did not improve from 0.72951\n",
            "16/16 [==============================] - 2s 148ms/step - loss: 0.6192 - accuracy: 0.7510 - val_loss: 0.5941 - val_accuracy: 0.6475\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4238 - accuracy: 0.9354\n",
            "Epoch 3: val_accuracy improved from 0.72951 to 0.86066, saving model to CNN_best_weightds.03-0.8607.hdf1\n",
            "16/16 [==============================] - 3s 186ms/step - loss: 0.4217 - accuracy: 0.9362 - val_loss: 0.4178 - val_accuracy: 0.8607\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9937\n",
            "Epoch 4: val_accuracy improved from 0.86066 to 0.88525, saving model to CNN_best_weightds.04-0.8852.hdf1\n",
            "16/16 [==============================] - 3s 193ms/step - loss: 0.1498 - accuracy: 0.9938 - val_loss: 0.2599 - val_accuracy: 0.8852\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.88525 to 0.89344, saving model to CNN_best_weightds.05-0.8934.hdf1\n",
            "16/16 [==============================] - 4s 232ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 0.8934\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0579 - accuracy: 0.9729\n",
            "Epoch 1: val_accuracy improved from 0.89344 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 208ms/step - loss: 0.0573 - accuracy: 0.9733 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 108ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.8011e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 8.7545e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 8.7078e-04 - accuracy: 1.0000 - val_loss: 5.6311e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 6.2294e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 156ms/step - loss: 6.2294e-04 - accuracy: 1.0000 - val_loss: 4.8140e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 5.1462e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 5.1462e-04 - accuracy: 1.0000 - val_loss: 4.1918e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.0836e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 173ms/step - loss: 4.0836e-04 - accuracy: 1.0000 - val_loss: 3.6667e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.0637e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 4.0805e-04 - accuracy: 1.0000 - val_loss: 2.5014e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.3668e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 3.3395e-04 - accuracy: 1.0000 - val_loss: 2.2298e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.9870e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 3.0287e-04 - accuracy: 1.0000 - val_loss: 2.0019e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3750e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 2.3516e-04 - accuracy: 1.0000 - val_loss: 1.7817e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1820e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 2.1871e-04 - accuracy: 1.0000 - val_loss: 1.6120e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.8728e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 1.8728e-04 - accuracy: 1.0000 - val_loss: 1.1550e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.7344e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 185ms/step - loss: 1.7344e-04 - accuracy: 1.0000 - val_loss: 1.0538e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.4941e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 183ms/step - loss: 1.4941e-04 - accuracy: 1.0000 - val_loss: 9.6602e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.3700e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 1.3681e-04 - accuracy: 1.0000 - val_loss: 8.8649e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3162e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 140ms/step - loss: 1.3162e-04 - accuracy: 1.0000 - val_loss: 8.1621e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_gHneYs7cR3",
        "outputId": "22e5e4c5-b514-4301-8f70-5ffbb10c8c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_16 (Embedding)    (None, 100, 200)             2000000   ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)          (None, 99, 100)              40100     ['embedding_16[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)          (None, 98, 100)              60100     ['embedding_16[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)          (None, 97, 100)              80100     ['embedding_16[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_36 (G  (None, 100)                  0         ['conv1d_36[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_37 (G  (None, 100)                  0         ['conv1d_37[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_38 (G  (None, 100)                  0         ['conv1d_38[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_36[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_37[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_38[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 256)                  77056     ['concatenate_12[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 256)                  0         ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 1)                    257       ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 1)                    0         ['dense_33[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.0001 y dropout = 0.3\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.3)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.0001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MqKFOgu7ork",
        "outputId": "a251a398-625a-4929-c52f-cc5470d133e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6768 - accuracy: 0.5417\n",
            "Epoch 1: val_accuracy improved from -inf to 0.62295, saving model to CNN_best_weightds.01-0.6230.hdf1\n",
            "16/16 [==============================] - 7s 227ms/step - loss: 0.6765 - accuracy: 0.5412 - val_loss: 0.6513 - val_accuracy: 0.6230\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5659 - accuracy: 0.8188\n",
            "Epoch 2: val_accuracy improved from 0.62295 to 0.89344, saving model to CNN_best_weightds.02-0.8934.hdf1\n",
            "16/16 [==============================] - 3s 192ms/step - loss: 0.5652 - accuracy: 0.8210 - val_loss: 0.5490 - val_accuracy: 0.8934\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3316 - accuracy: 0.9812\n",
            "Epoch 3: val_accuracy improved from 0.89344 to 0.90984, saving model to CNN_best_weightds.03-0.9098.hdf1\n",
            "16/16 [==============================] - 3s 196ms/step - loss: 0.3326 - accuracy: 0.9774 - val_loss: 0.3456 - val_accuracy: 0.9098\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0943 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.90984\n",
            "16/16 [==============================] - 2s 124ms/step - loss: 0.0942 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9098\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 0.90984\n",
            "16/16 [==============================] - 3s 175ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 0.9016\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9794\n",
            "Epoch 1: val_accuracy improved from 0.90984 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 4s 233ms/step - loss: 0.0542 - accuracy: 0.9794 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 154ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.6517e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 7.5692e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 137ms/step - loss: 7.5692e-04 - accuracy: 1.0000 - val_loss: 4.1162e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 5.5708e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 181ms/step - loss: 5.5708e-04 - accuracy: 1.0000 - val_loss: 3.1404e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.0154e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 187ms/step - loss: 4.0154e-04 - accuracy: 1.0000 - val_loss: 2.4780e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.1819e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 131ms/step - loss: 3.1522e-04 - accuracy: 1.0000 - val_loss: 2.0260e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6108e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 2.5920e-04 - accuracy: 1.0000 - val_loss: 1.2807e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0161e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 2.0161e-04 - accuracy: 1.0000 - val_loss: 1.0574e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0212e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 2.0102e-04 - accuracy: 1.0000 - val_loss: 9.0420e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.5767e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 1.6050e-04 - accuracy: 1.0000 - val_loss: 7.5717e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3882e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 1.3882e-04 - accuracy: 1.0000 - val_loss: 6.5678e-05 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3207e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 190ms/step - loss: 1.3207e-04 - accuracy: 1.0000 - val_loss: 4.3335e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.0778e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 1.0667e-04 - accuracy: 1.0000 - val_loss: 3.7581e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.0844e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 1.0747e-04 - accuracy: 1.0000 - val_loss: 3.2946e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 9.0472e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 9.0957e-05 - accuracy: 1.0000 - val_loss: 2.8956e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 6.9555e-05 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 6.8787e-05 - accuracy: 1.0000 - val_loss: 2.6071e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w1ujVT47qaO",
        "outputId": "b22024ca-ab20-4e58-c036-a69d78126fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_14 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_17 (Embedding)    (None, 100, 200)             2000000   ['input_14[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)          (None, 99, 100)              40100     ['embedding_17[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)          (None, 98, 100)              60100     ['embedding_17[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)          (None, 97, 100)              80100     ['embedding_17[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_39 (G  (None, 100)                  0         ['conv1d_39[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_40 (G  (None, 100)                  0         ['conv1d_40[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_41 (G  (None, 100)                  0         ['conv1d_41[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_39[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_40[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_41[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_34 (Dense)            (None, 256)                  77056     ['concatenate_13[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 256)                  0         ['dense_34[0][0]']            \n",
            "                                                                                                  \n",
            " dense_35 (Dense)            (None, 1)                    257       ['dropout_13[0][0]']          \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 1)                    0         ['dense_35[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.001 y dropout = 0.3\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.3)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpJt-6tr7y3y",
        "outputId": "df977880-f5e3-4f17-9259-c824a8e03216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6805 - accuracy: 0.5729\n",
            "Epoch 1: val_accuracy improved from -inf to 0.73770, saving model to CNN_best_weightds.01-0.7377.hdf1\n",
            "16/16 [==============================] - 5s 204ms/step - loss: 0.6804 - accuracy: 0.5761 - val_loss: 0.6630 - val_accuracy: 0.7377\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5870 - accuracy: 0.8021\n",
            "Epoch 2: val_accuracy improved from 0.73770 to 0.83607, saving model to CNN_best_weightds.02-0.8361.hdf1\n",
            "16/16 [==============================] - 3s 189ms/step - loss: 0.5860 - accuracy: 0.8045 - val_loss: 0.5563 - val_accuracy: 0.8361\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.9506\n",
            "Epoch 3: val_accuracy improved from 0.83607 to 0.88525, saving model to CNN_best_weightds.03-0.8852.hdf1\n",
            "16/16 [==============================] - 4s 266ms/step - loss: 0.3654 - accuracy: 0.9506 - val_loss: 0.3752 - val_accuracy: 0.8852\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9959\n",
            "Epoch 4: val_accuracy improved from 0.88525 to 0.90984, saving model to CNN_best_weightds.04-0.9098.hdf1\n",
            "16/16 [==============================] - 4s 288ms/step - loss: 0.1369 - accuracy: 0.9959 - val_loss: 0.2308 - val_accuracy: 0.9098\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.90984 to 0.91803, saving model to CNN_best_weightds.05-0.9180.hdf1\n",
            "16/16 [==============================] - 3s 197ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 0.9180\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0485 - accuracy: 0.9812\n",
            "Epoch 1: val_accuracy improved from 0.91803 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 4s 252ms/step - loss: 0.0481 - accuracy: 0.9815 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 181ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 151ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.8892e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 181ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 7.5102e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 9.0730e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 9.0730e-04 - accuracy: 1.0000 - val_loss: 6.5908e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 7.3120e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 7.4489e-04 - accuracy: 1.0000 - val_loss: 5.8829e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 6.3311e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 6.2865e-04 - accuracy: 1.0000 - val_loss: 5.3757e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.6940e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 5.6538e-04 - accuracy: 1.0000 - val_loss: 3.5821e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.9857e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 4.9508e-04 - accuracy: 1.0000 - val_loss: 3.2837e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.7229e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 4.6961e-04 - accuracy: 1.0000 - val_loss: 3.0150e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.1576e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 139ms/step - loss: 4.1576e-04 - accuracy: 1.0000 - val_loss: 2.7848e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 3.5936e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 3.5936e-04 - accuracy: 1.0000 - val_loss: 2.5883e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.6172e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 3.5875e-04 - accuracy: 1.0000 - val_loss: 1.8490e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.8966e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 2.8966e-04 - accuracy: 1.0000 - val_loss: 1.7276e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.8897e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 2.8897e-04 - accuracy: 1.0000 - val_loss: 1.6230e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5071e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 159ms/step - loss: 2.4945e-04 - accuracy: 1.0000 - val_loss: 1.5230e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2856e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 2.2787e-04 - accuracy: 1.0000 - val_loss: 1.4355e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_9RvoO471Ir",
        "outputId": "d4f333ec-db0e-4b1d-b9a8-ed8d01e4a998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_18 (Embedding)    (None, 100, 200)             2000000   ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)          (None, 99, 100)              40100     ['embedding_18[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)          (None, 98, 100)              60100     ['embedding_18[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)          (None, 97, 100)              80100     ['embedding_18[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_42 (G  (None, 100)                  0         ['conv1d_42[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_43 (G  (None, 100)                  0         ['conv1d_43[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_44 (G  (None, 100)                  0         ['conv1d_44[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_42[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_43[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_44[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_36 (Dense)            (None, 256)                  77056     ['concatenate_14[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 256)                  0         ['dense_36[0][0]']            \n",
            "                                                                                                  \n",
            " dense_37 (Dense)            (None, 1)                    257       ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 1)                    0         ['dense_37[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.01 y dropout = 0.3\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.3)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.01)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0xKq80s77-L",
        "outputId": "24047aab-bd61-4b96-e036-766c541e512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6800 - accuracy: 0.5562\n",
            "Epoch 1: val_accuracy improved from -inf to 0.56557, saving model to CNN_best_weightds.01-0.5656.hdf1\n",
            "16/16 [==============================] - 5s 211ms/step - loss: 0.6808 - accuracy: 0.5535 - val_loss: 0.6702 - val_accuracy: 0.5656\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6026 - accuracy: 0.7437\n",
            "Epoch 2: val_accuracy improved from 0.56557 to 0.88525, saving model to CNN_best_weightds.02-0.8852.hdf1\n",
            "16/16 [==============================] - 3s 197ms/step - loss: 0.6018 - accuracy: 0.7449 - val_loss: 0.5811 - val_accuracy: 0.8852\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.9712\n",
            "Epoch 3: val_accuracy did not improve from 0.88525\n",
            "16/16 [==============================] - 3s 183ms/step - loss: 0.3856 - accuracy: 0.9712 - val_loss: 0.3762 - val_accuracy: 0.8852\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9979\n",
            "Epoch 4: val_accuracy improved from 0.88525 to 0.90164, saving model to CNN_best_weightds.04-0.9016.hdf1\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.1144 - accuracy: 0.9979 - val_loss: 0.2208 - val_accuracy: 0.9016\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.90164 to 0.91803, saving model to CNN_best_weightds.05-0.9180.hdf1\n",
            "16/16 [==============================] - 3s 194ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.2053 - val_accuracy: 0.9180\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0501 - accuracy: 0.9792\n",
            "Epoch 1: val_accuracy improved from 0.91803 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 204ms/step - loss: 0.0495 - accuracy: 0.9794 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 175ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 186ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 159ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 217ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 6.9429e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 8.8573e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 8.8573e-04 - accuracy: 1.0000 - val_loss: 5.9594e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 6.9792e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 153ms/step - loss: 6.9992e-04 - accuracy: 1.0000 - val_loss: 5.0823e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.7151e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 5.7235e-04 - accuracy: 1.0000 - val_loss: 4.4606e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.4052e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 4.3895e-04 - accuracy: 1.0000 - val_loss: 4.0092e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.8583e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 4.8583e-04 - accuracy: 1.0000 - val_loss: 2.4996e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.6144e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 3.6043e-04 - accuracy: 1.0000 - val_loss: 2.2411e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.6052e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 3.5943e-04 - accuracy: 1.0000 - val_loss: 2.0338e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.9685e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 2.9612e-04 - accuracy: 1.0000 - val_loss: 1.8611e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.7068e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 2.7008e-04 - accuracy: 1.0000 - val_loss: 1.7177e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4164e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 2.5034e-04 - accuracy: 1.0000 - val_loss: 1.3443e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1991e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 135ms/step - loss: 2.1991e-04 - accuracy: 1.0000 - val_loss: 1.2284e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9977e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 1.9977e-04 - accuracy: 1.0000 - val_loss: 1.1413e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9341e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 186ms/step - loss: 1.9341e-04 - accuracy: 1.0000 - val_loss: 1.0619e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.6642e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 133ms/step - loss: 1.6670e-04 - accuracy: 1.0000 - val_loss: 9.9679e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MEmIjrJ79ul",
        "outputId": "b982b89d-151f-4c3e-abf0-0de52d69f1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_16 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_19 (Embedding)    (None, 100, 200)             2000000   ['input_16[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)          (None, 99, 100)              40100     ['embedding_19[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)          (None, 98, 100)              60100     ['embedding_19[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)          (None, 97, 100)              80100     ['embedding_19[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_45 (G  (None, 100)                  0         ['conv1d_45[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_46 (G  (None, 100)                  0         ['conv1d_46[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_47 (G  (None, 100)                  0         ['conv1d_47[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_45[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_46[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_47[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_38 (Dense)            (None, 256)                  77056     ['concatenate_15[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 256)                  0         ['dense_38[0][0]']            \n",
            "                                                                                                  \n",
            " dense_39 (Dense)            (None, 1)                    257       ['dropout_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 1)                    0         ['dense_39[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.0001 y dropout = 0.4\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.0001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J-nG9zK8KpN",
        "outputId": "dc12c5f8-8e86-4a25-f4c7-d811f4d099e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6846 - accuracy: 0.5521\n",
            "Epoch 1: val_accuracy improved from -inf to 0.63115, saving model to CNN_best_weightds.01-0.6311.hdf1\n",
            "16/16 [==============================] - 5s 211ms/step - loss: 0.6842 - accuracy: 0.5556 - val_loss: 0.6659 - val_accuracy: 0.6311\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6187 - accuracy: 0.6917\n",
            "Epoch 2: val_accuracy improved from 0.63115 to 0.70492, saving model to CNN_best_weightds.02-0.7049.hdf1\n",
            "16/16 [==============================] - 3s 194ms/step - loss: 0.6178 - accuracy: 0.6934 - val_loss: 0.5967 - val_accuracy: 0.7049\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.8930\n",
            "Epoch 3: val_accuracy improved from 0.70492 to 0.90164, saving model to CNN_best_weightds.03-0.9016.hdf1\n",
            "16/16 [==============================] - 4s 277ms/step - loss: 0.4528 - accuracy: 0.8930 - val_loss: 0.4399 - val_accuracy: 0.9016\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9938\n",
            "Epoch 4: val_accuracy did not improve from 0.90164\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.1868 - accuracy: 0.9938 - val_loss: 0.2604 - val_accuracy: 0.9016\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0431 - accuracy: 0.9979\n",
            "Epoch 5: val_accuracy did not improve from 0.90164\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0428 - accuracy: 0.9979 - val_loss: 0.2606 - val_accuracy: 0.8607\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0699 - accuracy: 0.9688\n",
            "Epoch 1: val_accuracy improved from 0.90164 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 195ms/step - loss: 0.0692 - accuracy: 0.9691 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 152ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 7.8337e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.3671e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 8.2091e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 183ms/step - loss: 8.2091e-04 - accuracy: 1.0000 - val_loss: 5.3551e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 6.7184e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 185ms/step - loss: 6.7184e-04 - accuracy: 1.0000 - val_loss: 4.5847e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.7346e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 142ms/step - loss: 5.7498e-04 - accuracy: 1.0000 - val_loss: 4.0266e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.0719e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 5.0956e-04 - accuracy: 1.0000 - val_loss: 2.6139e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.1599e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 109ms/step - loss: 4.1307e-04 - accuracy: 1.0000 - val_loss: 2.2952e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.2817e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 4.2348e-04 - accuracy: 1.0000 - val_loss: 2.0456e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.5971e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 3.5696e-04 - accuracy: 1.0000 - val_loss: 1.8354e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.4529e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 3.4446e-04 - accuracy: 1.0000 - val_loss: 1.6765e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.9486e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 2.9486e-04 - accuracy: 1.0000 - val_loss: 1.1321e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5730e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 2.5730e-04 - accuracy: 1.0000 - val_loss: 1.0227e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4383e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 2.4383e-04 - accuracy: 1.0000 - val_loss: 9.2369e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0904e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 2.0701e-04 - accuracy: 1.0000 - val_loss: 8.4600e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0044e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 1.9894e-04 - accuracy: 1.0000 - val_loss: 7.8090e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dEEYB6v8m0b",
        "outputId": "9a16834d-f77d-46b2-d3fa-3137b880f5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_20 (Embedding)    (None, 100, 200)             2000000   ['input_17[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)          (None, 99, 100)              40100     ['embedding_20[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)          (None, 98, 100)              60100     ['embedding_20[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)          (None, 97, 100)              80100     ['embedding_20[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_48 (G  (None, 100)                  0         ['conv1d_48[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_49 (G  (None, 100)                  0         ['conv1d_49[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_50 (G  (None, 100)                  0         ['conv1d_50[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_48[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_49[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_50[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_40 (Dense)            (None, 256)                  77056     ['concatenate_16[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 256)                  0         ['dense_40[0][0]']            \n",
            "                                                                                                  \n",
            " dense_41 (Dense)            (None, 1)                    257       ['dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 1)                    0         ['dense_41[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.001 y dropout = 0.4\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.001)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iSXunKv8rqN",
        "outputId": "165aa0b9-a301-47cc-8f17-27a0d24502a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6925 - accuracy: 0.5708\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85246, saving model to CNN_best_weightds.01-0.8525.hdf1\n",
            "16/16 [==============================] - 5s 246ms/step - loss: 0.6929 - accuracy: 0.5658 - val_loss: 0.6687 - val_accuracy: 0.8525\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.7346\n",
            "Epoch 2: val_accuracy did not improve from 0.85246\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 0.6255 - accuracy: 0.7346 - val_loss: 0.6065 - val_accuracy: 0.7131\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4671 - accuracy: 0.9708\n",
            "Epoch 3: val_accuracy improved from 0.85246 to 0.88525, saving model to CNN_best_weightds.03-0.8852.hdf1\n",
            "16/16 [==============================] - 3s 224ms/step - loss: 0.4660 - accuracy: 0.9691 - val_loss: 0.4593 - val_accuracy: 0.8852\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9897\n",
            "Epoch 4: val_accuracy improved from 0.88525 to 0.89344, saving model to CNN_best_weightds.04-0.8934.hdf1\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 0.2217 - accuracy: 0.9897 - val_loss: 0.2779 - val_accuracy: 0.8934\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0511 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 0.89344\n",
            "16/16 [==============================] - 2s 133ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.8770\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0660 - accuracy: 0.9708\n",
            "Epoch 1: val_accuracy improved from 0.89344 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 204ms/step - loss: 0.0661 - accuracy: 0.9712 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 172ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 186ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 9.2459e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 7.1916e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 9.0891e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 9.0891e-04 - accuracy: 1.0000 - val_loss: 5.6752e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 7.4742e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 7.4105e-04 - accuracy: 1.0000 - val_loss: 4.6134e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.9115e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 5.9135e-04 - accuracy: 1.0000 - val_loss: 3.7025e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.5819e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 4.5583e-04 - accuracy: 1.0000 - val_loss: 2.0498e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.3841e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 4.3790e-04 - accuracy: 1.0000 - val_loss: 1.6763e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 3.7353e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 3.7353e-04 - accuracy: 1.0000 - val_loss: 1.4288e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6968e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 2.7104e-04 - accuracy: 1.0000 - val_loss: 1.2053e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.9706e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 2.9706e-04 - accuracy: 1.0000 - val_loss: 9.9307e-05 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4509e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 2.4509e-04 - accuracy: 1.0000 - val_loss: 7.3607e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1657e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 2.1657e-04 - accuracy: 1.0000 - val_loss: 6.3496e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.7549e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 1.7382e-04 - accuracy: 1.0000 - val_loss: 5.6106e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.6669e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 1.6690e-04 - accuracy: 1.0000 - val_loss: 5.1558e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.5650e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 1.5491e-04 - accuracy: 1.0000 - val_loss: 4.6432e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbCff4o-8svA",
        "outputId": "8a344953-d5d9-4afc-a069-a82620fb6c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)       [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_21 (Embedding)    (None, 100, 200)             2000000   ['input_18[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)          (None, 99, 100)              40100     ['embedding_21[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)          (None, 98, 100)              60100     ['embedding_21[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)          (None, 97, 100)              80100     ['embedding_21[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_51 (G  (None, 100)                  0         ['conv1d_51[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_52 (G  (None, 100)                  0         ['conv1d_52[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_53 (G  (None, 100)                  0         ['conv1d_53[0][0]']           \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenat  (None, 300)                  0         ['global_max_pooling1d_51[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'global_max_pooling1d_52[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'global_max_pooling1d_53[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_42 (Dense)            (None, 256)                  77056     ['concatenate_17[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 256)                  0         ['dense_42[0][0]']            \n",
            "                                                                                                  \n",
            " dense_43 (Dense)            (None, 1)                    257       ['dropout_17[0][0]']          \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 1)                    0         ['dense_43[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257613 (8.61 MB)\n",
            "Trainable params: 2257613 (8.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#lr = 0.01 y dropout = 0.4\n",
        "tweet_input = Input(shape=(maximocam,), dtype='int32')\n",
        "\n",
        "tweet_encoder = Embedding(10000, 200, input_length=maximocam)(tweet_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "bigram_branch = GlobalMaxPool1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "trigram_branch = GlobalMaxPool1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu',strides=1)(tweet_encoder)\n",
        "fourgram_branch = GlobalMaxPool1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation= 'relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "merged = Dense(1)(merged)\n",
        "output = Activation('sigmoid')(merged)\n",
        "opt = Adam(lr=0.01)\n",
        "model = Model(inputs=[tweet_input],outputs=[output])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = opt,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gHxtdgI8xrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7eaae4b-493f-4c50-98a1-19d0c559f849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6852 - accuracy: 0.5542\n",
            "Epoch 1: val_accuracy improved from -inf to 0.76230, saving model to CNN_best_weightds.01-0.7623.hdf1\n",
            "16/16 [==============================] - 5s 210ms/step - loss: 0.6853 - accuracy: 0.5576 - val_loss: 0.6642 - val_accuracy: 0.7623\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6133 - accuracy: 0.6729\n",
            "Epoch 2: val_accuracy did not improve from 0.76230\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.6119 - accuracy: 0.6770 - val_loss: 0.5916 - val_accuracy: 0.7623\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4294 - accuracy: 0.9333\n",
            "Epoch 3: val_accuracy improved from 0.76230 to 0.85246, saving model to CNN_best_weightds.03-0.8525.hdf1\n",
            "16/16 [==============================] - 3s 211ms/step - loss: 0.4275 - accuracy: 0.9342 - val_loss: 0.4369 - val_accuracy: 0.8525\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9897\n",
            "Epoch 4: val_accuracy improved from 0.85246 to 0.91803, saving model to CNN_best_weightds.04-0.9180.hdf1\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.1737 - accuracy: 0.9897 - val_loss: 0.2616 - val_accuracy: 0.9180\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9979\n",
            "Epoch 5: val_accuracy did not improve from 0.91803\n",
            "16/16 [==============================] - 2s 131ms/step - loss: 0.0389 - accuracy: 0.9979 - val_loss: 0.2255 - val_accuracy: 0.8525\n",
            "---------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0536 - accuracy: 0.9750\n",
            "Epoch 1: val_accuracy improved from 0.91803 to 1.00000, saving model to CNN_best_weightds.01-1.0000.hdf1\n",
            "16/16 [==============================] - 3s 203ms/step - loss: 0.0540 - accuracy: 0.9753 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 173ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 135ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 6.9297e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 9.6348e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 189ms/step - loss: 9.6348e-04 - accuracy: 1.0000 - val_loss: 5.6656e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 7.8321e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 184ms/step - loss: 7.8321e-04 - accuracy: 1.0000 - val_loss: 4.6447e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.9259e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 157ms/step - loss: 5.9142e-04 - accuracy: 1.0000 - val_loss: 3.9644e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 5.1134e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 5.1067e-04 - accuracy: 1.0000 - val_loss: 3.3520e-04 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 4.2036e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 4.2036e-04 - accuracy: 1.0000 - val_loss: 1.9847e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 3.3483e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 3.3483e-04 - accuracy: 1.0000 - val_loss: 1.6141e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6118e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 2.5983e-04 - accuracy: 1.0000 - val_loss: 1.3506e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5409e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 2.5186e-04 - accuracy: 1.0000 - val_loss: 1.1312e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4099e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 2.5050e-04 - accuracy: 1.0000 - val_loss: 9.4575e-05 - val_accuracy: 1.0000\n",
            "---------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.7783e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 1.8184e-04 - accuracy: 1.0000 - val_loss: 6.1868e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.7576e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 1.7416e-04 - accuracy: 1.0000 - val_loss: 5.3823e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.6180e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 1.6055e-04 - accuracy: 1.0000 - val_loss: 4.6655e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.4150e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 1.4086e-04 - accuracy: 1.0000 - val_loss: 4.1756e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.2414e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 1.2416e-04 - accuracy: 1.0000 - val_loss: 3.8435e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weightds.{epoch:02d}-{val_accuracy:.4f}.hdf1\"\n",
        "checkpoint =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "fold_no=1\n",
        "for train_index, test_index in kf.split(x_train_seq, y_train):\n",
        "  print('---------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  x_tr, x_te = x_new[train_index], x_new[test_index]\n",
        "  y_tr, y_te = y_new[train_index], y_new[test_index]\n",
        "\n",
        "  model.fit(x_tr, y_tr, batch_size=32, epochs=5,\n",
        "            validation_data=(x_te, y_te), callbacks =[checkpoint])\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOcH9mSU9eGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37f6498-3817-4807-a75c-4fdbf0648a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0053 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0053399489261209965, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "#Cargar modelo segun el nombre del archivo anterior con el modelo de mejor peso\n",
        "from keras.models import load_model\n",
        "loaded_CNN_model = load_model('CNN_best_weightds.01-1.0000.hdf1')\n",
        "loaded_CNN_model.evaluate(x=x_te, y=y_te)\n",
        "\n",
        "#[perdida aproximada, precision aproximada]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O-7kpL893zU"
      },
      "source": [
        "**Evaluación final del modelo con datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-kvaU7t9-vz"
      },
      "outputs": [],
      "source": [
        "#Tokenizacion de las palabras en tokens y establece longitud de la secuencia segun la longitud\n",
        "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_test_seq = pad_sequences(sequences_test, maxlen=maximocam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDxHnJTA-OrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff762a0-6559-4cac-d4a3-0d9002d1bf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 52ms/step - loss: 0.1812 - accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18120814859867096, 0.8888888955116272]"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ],
      "source": [
        "#Carga de modelo CNN\n",
        "loaded_CNN_model.evaluate(x=x_test_seq, y=y_test)\n",
        "#[perdida aproximada, precision aproximada]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHHao8bo-Vp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c20f9ab-a7af-409f-cb0a-2bc9c2951d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 42ms/step\n",
            "[1]\n",
            "[1, 0]\n",
            "[1, 0, 1]\n",
            "[1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "#Convierte la prediccion del modelo en 1 o 0\n",
        "y_temp = loaded_CNN_model.predict(x_test_seq)\n",
        "yhat_cnn = []\n",
        "for i in range(0, len(y_temp)):\n",
        "  if(y_temp[i] > 1-y_temp[i]):\n",
        "    yhat_cnn.append(1)\n",
        "  else:\n",
        "    yhat_cnn.append(0)\n",
        "  print(yhat_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-3UH8qq-tv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f08cd0-5e9b-46ee-b45a-65b98e89933d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[85,  7],\n",
              "       [10, 51]])"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Matriz de contingencia para evaluar el rendimiento\n",
        "#[[VP=verdadero positivo, FP=Falso positivo],\n",
        "#[FN=Falso negativo, VP=Verdadero negativo]]\n",
        "\n",
        "confusion_matrix(y_test, yhat_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMXuNx4U-1T8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "2637af82-ead7-4ecd-be59-0d040764bdee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 222
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Graficando la ruta ROC\n",
        "# Si el AUC = 0.7 entonces es un 70% de que pueda distinguir la clasificacion\n",
        "# Si el AUC = 0.5 entonces no puede distinguir eficientemente\n",
        "fpr_cnn, tpr_cnn, threshold = roc_curve(y_test, yhat_cnn)\n",
        "roc_auc_nn = auc(fpr_cnn, tpr_cnn)\n",
        "plt.figure(figsize=(8,7))\n",
        "plt.plot(fpr_cnn, tpr_cnn, label='w2v-CNN (area = %0.3f)' % roc_auc_nn, linewidth=2)\n",
        "\n",
        "plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Ratio de falso positivo', fontsize= 18)\n",
        "plt.ylabel('Ratio de falso negativo', fontsize= 18)\n",
        "plt.title('Operacion positiva: es positivo', fontsize=18)\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNt3pcTSAhL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f513e0-fac9-471f-98f9-f4db8a69ee47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.07608696, 1.        ]),\n",
              " array([0.        , 0.83606557, 1.        ]),\n",
              " 0.8799893086243763)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "#Muestra los valores de la curva ROC\n",
        "#fpr_cnn es falso positivo en diferentes cortes del cuadro\n",
        "#tpr_cnn es verdadero positivo en diferentes cortes del cuadro\n",
        "#roc_auc_nn area de la Curva ROC\n",
        "fpr_cnn, tpr_cnn, roc_auc_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4oGOzjZAvis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab073902-8b85-4cc0-efe8-d3e40a5e246c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla de prediccion de los datos con la red neuronal\n",
            "                                                Frases  Pred.  Pred.Red\n",
            "201  I personally don’t respect a lot of people’s o...      1         1\n",
            "37   I have obviously chosen not to include any of ...      0         0\n",
            "758  If someone codes an AI to play a competitive g...      0         1\n",
            "470  Instead of Green Skull, all we're left with is...      0         0\n",
            "575  I agree. I have been honing my skills with ai....      1         1\n",
            "..                                                 ...    ...       ...\n",
            "33   I have written another post about AI generated...      0         0\n",
            "47   QRT and release something in the IA gallery th...      1         1\n",
            "15   Just to add, this is the first of many to come...      1         0\n",
            "516  Graphic Designers are going to do just fine if...      1         0\n",
            "123                                Night drive with ia      1         1\n",
            "\n",
            "[153 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#y_test se  muestra la columna de los valores con 1 o 0 (positivo/negativo)\n",
        "#y_temp se muestra la instancia de una probabilidad que pertenezca a la clase positiva\n",
        "#yhat_cnn prediccion con umbral que muestra 1 o 0 (positivo/negativo)\n",
        "#y_temp.shape muestra la matriz en (filas/columnas)\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Tabla de prediccion de los datos con la red neuronal\")\n",
        "df = pd.DataFrame({'Frases': x_test,'Pred.': y_test,'Pred.Red': yhat_cnn})\n",
        "\n",
        "df.index = df.index + 1  # Sumar 1 a los índices para iniciar desde 1 en lugar de 0\n",
        "\n",
        "\n",
        "# Mostrar el DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iORTwxsJLmPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55b0435-fd89-4256-c84d-02137241edba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidad que sea positivo o negativo segun la CNN\n",
            "[[9.8189157e-01]\n",
            " [3.6690473e-03]\n",
            " [5.9202766e-01]\n",
            " [1.0554906e-02]\n",
            " [9.9649590e-01]\n",
            " [9.4487285e-03]\n",
            " [4.7833426e-04]\n",
            " [9.9540633e-01]\n",
            " [6.8553969e-02]\n",
            " [1.0851822e-03]\n",
            " [5.3435545e-02]\n",
            " [2.2065213e-03]\n",
            " [6.7147329e-02]\n",
            " [5.6756530e-03]\n",
            " [3.6690473e-03]\n",
            " [2.8868115e-03]\n",
            " [1.9413678e-03]\n",
            " [3.1238249e-01]\n",
            " [9.9118179e-01]\n",
            " [9.0548390e-01]\n",
            " [9.6895647e-01]\n",
            " [9.7156197e-01]\n",
            " [2.2798970e-02]\n",
            " [9.9027628e-01]\n",
            " [1.9686786e-03]\n",
            " [1.8343462e-02]\n",
            " [1.6752995e-03]\n",
            " [4.8648185e-01]\n",
            " [9.8520029e-01]\n",
            " [3.1509984e-02]\n",
            " [5.3435545e-02]\n",
            " [9.8268974e-01]\n",
            " [2.6534356e-03]\n",
            " [3.1637123e-03]\n",
            " [3.8589453e-03]\n",
            " [9.9320590e-01]\n",
            " [9.9046457e-01]\n",
            " [9.9263579e-01]\n",
            " [2.9008922e-03]\n",
            " [9.8189157e-01]\n",
            " [1.5485922e-03]\n",
            " [9.5622379e-01]\n",
            " [5.6756530e-03]\n",
            " [2.2214385e-02]\n",
            " [2.9081453e-03]\n",
            " [8.2990009e-01]\n",
            " [1.7551599e-03]\n",
            " [9.9244171e-01]\n",
            " [9.9485648e-01]\n",
            " [1.9184992e-02]\n",
            " [9.7576761e-01]\n",
            " [7.9390202e-03]\n",
            " [4.0907110e-03]\n",
            " [9.8991364e-01]\n",
            " [9.9644774e-01]\n",
            " [2.1870019e-01]\n",
            " [4.4836316e-02]\n",
            " [9.5622379e-01]\n",
            " [2.5597811e-03]\n",
            " [3.1923770e-03]\n",
            " [9.8135579e-01]\n",
            " [1.0995899e-02]\n",
            " [6.0746428e-03]\n",
            " [9.9128252e-04]\n",
            " [5.1382747e-03]\n",
            " [8.1503689e-02]\n",
            " [3.6690473e-03]\n",
            " [1.1854584e-02]\n",
            " [1.2212658e-02]\n",
            " [4.2676306e-03]\n",
            " [1.7573384e-03]\n",
            " [9.5782232e-01]\n",
            " [3.1238249e-01]\n",
            " [9.9662852e-01]\n",
            " [9.8676729e-01]\n",
            " [5.2261580e-04]\n",
            " [9.9624532e-01]\n",
            " [1.9398505e-03]\n",
            " [9.9539131e-01]\n",
            " [5.5276896e-03]\n",
            " [2.7730528e-01]\n",
            " [6.0363053e-03]\n",
            " [1.4971523e-03]\n",
            " [9.9554390e-01]\n",
            " [8.6991061e-03]\n",
            " [9.7867650e-01]\n",
            " [7.1370757e-01]\n",
            " [5.2956771e-03]\n",
            " [2.5597811e-03]\n",
            " [5.9202766e-01]\n",
            " [3.6177066e-01]\n",
            " [6.1552674e-03]\n",
            " [2.9390503e-03]\n",
            " [1.8386886e-02]\n",
            " [3.9761355e-03]\n",
            " [3.8805243e-02]\n",
            " [9.8187667e-01]\n",
            " [1.2824729e-01]\n",
            " [9.9816316e-01]\n",
            " [9.8189157e-01]\n",
            " [3.0204186e-01]\n",
            " [1.7915189e-03]\n",
            " [9.6555817e-01]\n",
            " [1.5220752e-03]\n",
            " [1.9323898e-03]\n",
            " [9.9095732e-01]\n",
            " [9.6720306e-04]\n",
            " [9.9094164e-01]\n",
            " [9.3112879e-02]\n",
            " [9.9653178e-01]\n",
            " [6.0662824e-01]\n",
            " [9.3333215e-01]\n",
            " [2.2664031e-02]\n",
            " [2.5597811e-03]\n",
            " [5.7060586e-04]\n",
            " [4.5414600e-01]\n",
            " [9.6460235e-01]\n",
            " [9.8749709e-01]\n",
            " [9.3333215e-01]\n",
            " [4.3656770e-02]\n",
            " [9.4900870e-01]\n",
            " [1.1181157e-02]\n",
            " [8.2990009e-01]\n",
            " [9.9414498e-01]\n",
            " [6.9816345e-03]\n",
            " [2.8432587e-03]\n",
            " [3.0204183e-01]\n",
            " [1.5992557e-03]\n",
            " [2.8868115e-03]\n",
            " [9.9086267e-01]\n",
            " [9.3112879e-02]\n",
            " [4.5414600e-01]\n",
            " [9.9621338e-01]\n",
            " [5.1010111e-03]\n",
            " [9.9299496e-01]\n",
            " [9.8982453e-01]\n",
            " [6.9015071e-04]\n",
            " [7.1370757e-01]\n",
            " [9.7715741e-01]\n",
            " [5.3912783e-03]\n",
            " [9.8454833e-01]\n",
            " [6.8553969e-02]\n",
            " [1.6495027e-03]\n",
            " [8.1334515e-03]\n",
            " [2.9738646e-03]\n",
            " [2.4796554e-01]\n",
            " [4.3328842e-03]\n",
            " [9.8959172e-01]\n",
            " [2.0601753e-02]\n",
            " [9.9321347e-01]\n",
            " [2.1870019e-01]\n",
            " [4.8648185e-01]\n",
            " [9.8436385e-01]]\n",
            "Tamaño de la matriz\n",
            "(153, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"Probabilidad que sea positivo o negativo segun la CNN\")\n",
        "print(y_temp)\n",
        "print(\"Tamaño de la matriz\")\n",
        "print(y_temp.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1Du3qCNRqqJ9xwZ2FyigV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}